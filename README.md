# PDF Chat Program - README

# üìö Chat with Multiple PDFs

This project allows you to **ask questions to multiple PDFs** using **LangChain**, **FAISS**, and **HuggingFace Transformer models**. It extracts text from PDFs, splits it into chunks, embeds the text using a sentence-transformer, and allows conversational retrieval using a large language model.

---

## Features

* Upload multiple PDFs and process them into an interactive conversational bot.
* Uses **HuggingFace pipelines locally** to avoid API limitations.
* Uses **FAISS** for efficient vector storage and retrieval.
* Maintains **chat history** for conversational context.

---

## Requirements

Python >= 3.9 (tested on **Mac M2 Air**).
Required packages are listed in `requirements.txt`.

> **Note:** FAISS may require special installation for ARM-based Macs (M1/M2). Use:
> `pip install faiss-cpu` (no GPU support needed).

---
## Usage

1. Run the Streamlit app:

```bash
streamlit run app.py
```

2. Upload PDFs using the sidebar.
3. Click **Process** to create the conversational vector store.
4. Ask questions in the input box, and the bot will respond using retrieved PDF content.

---

## Code Structure

* `app.py` ‚Üí Main Streamlit application.
* `htmlTemplates.py` ‚Üí Contains HTML templates for chat UI.
* `requirements.txt` ‚Üí Contains all Python dependencies.
* `venv/` ‚Üí Optional virtual environment.

**Key Functions:**

* `get_pdf_text(pdf_docs)` ‚Üí Extracts text from uploaded PDFs.
* `get_text_chunks(text)` ‚Üí Splits text into manageable chunks.
* `get_vectorstore(text_chunks)` ‚Üí Creates FAISS vector store from embeddings.
* `get_conversation_chain(vectorstore)` ‚Üí Initializes LangChain conversational retrieval chain.
* `handle_user_input(user_question)` ‚Üí Processes user input and returns the response.

---

## Why HuggingFacePipeline?

We use **HuggingFacePipeline** because it allows us to load transformer models locally using the `transformers` library without relying on a HuggingFaceHub endpoint.
This avoids errors like `Task not specified` and `StopIteration` which occur with HuggingFaceHub or HuggingFaceEndpoint when using unsupported or cloud-only models.

**Tokenizer Usage:**

The tokenizer is used internally by the HuggingFace pipeline to convert text input into tokens the model can understand. It ensures proper text encoding for the transformer model and controls input length (`max_length`) and output generation (`max_new_tokens`).

---

## Common Issues & Troubleshooting

### 1Ô∏è‚É£ HuggingFaceHub errors:

* **Error:** `Task not specified` or `NotImplementedError`
* **Solution:** Use **HuggingFacePipeline with a local transformers pipeline** instead.

### 2Ô∏è‚É£ HuggingFaceEndpoint errors:

* **Error:** `StopIteration` or `provider_helper` issues
* **Solution:** Use **local HuggingFace pipeline** (no cloud required).

### 3Ô∏è‚É£ HuggingFacePipeline errors:

### 4Ô∏è‚É£ FAISS installation on M1/M2 Macs:

* **Error:** Compilation fails on ARM architecture
* **Solution:** Install prebuilt CPU version:

### 5Ô∏è‚É£ Cache issues / large downloads:

* Transformers models and embeddings are cached in:
  `~/.cache/huggingface/`
* To clear space:

```bash
rm -rf ~/.cache/huggingface/
```

---
## References

* [LangChain Documentation](https://python.langchain.com/docs/)
* [HuggingFace Transformers Docs](https://huggingface.co/docs/transformers/index)
* [FAISS GitHub](https://github.com/facebookresearch/faiss)

---